{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_final.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23867e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8dd988",
   "metadata": {},
   "source": [
    "# Creating tables\n",
    "\n",
    "Table 1 - Employees (primary_key: employee_number)    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 4 - education: education_id + education_category  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 5 - education_field: education_field_id + education_field  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 7 - gender: gender_id + gender  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 10 - marital_status: marital_status_id + marital_status  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 12 - stock_option_level: stock_option_level_id + stock_option_level (numerica)\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Tables - Salaries  \n",
    "\n",
    "->Table 2 - Identificador de trabajos Tipologias de trabajo  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 2 - business_travel: business_travel_id + business_travel_category  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 11 - standard_hours: standard_hours_id + standard_hours_category     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table - Remote work\n",
    "\n",
    "-> Table 3 - scale_1_4: scale_id + scale_level  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; environment_satisfaction   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; job_satisfaction  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; relationship_satisfaction   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; job_involvement  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 13 - work_life_balance: work_life_balance_id + work_life_balance_category    \n",
    "\n",
    "-> Table 4 - Department_Role  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 3 - department: department_id + department_name  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 8 - job_level: job_level_id + job_level  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 9 - job_role: job_role_id + job_role  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28908598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2 - Work Typologies\n",
    "# Create list of the typologies characteristics\n",
    "categories1 = df['business_travel'].unique().tolist()\n",
    "categories2 = df['standard_hours'].unique().tolist()\n",
    "categories3 = df['remote_work'].unique().tolist()\n",
    "\n",
    "# Create DataFrames (df) based on the litsts\n",
    "work_typologies1 = pd.DataFrame({'category_travel': categories1})\n",
    "work_typologies2 = pd.DataFrame({'category_std_hours': categories2})\n",
    "work_typologies3 = pd.DataFrame({'category_remote': categories3})\n",
    "\n",
    "# Create a mapping order for the characteristics\n",
    "order_mapping1 = {'non_travel': 1, 'travel_rarely': 2, 'travel_frequently': 3, 'NaN': 4}\n",
    "order_mapping2 = {'full_time': 1, 'part_time': 2, 'NaN': 3}\n",
    "order_mapping3 = {'yes': 1, 'no': 2, 'NaN': 3}\n",
    "\n",
    "# Sort using custom mapping\n",
    "work_typologies1 = work_typologies1.assign(sort_order = lambda work_typologies1: work_typologies1['category_travel'].map(order_mapping1)).sort_values('sort_order').drop('sort_order', axis=1)\n",
    "work_typologies2 = work_typologies2.assign(sort_order = lambda work_typologies2: work_typologies2['category_std_hours'].map(order_mapping2)).sort_values('sort_order').drop('sort_order', axis=1)\n",
    "work_typologies3 = work_typologies3.assign(sort_order = lambda work_typologies3: work_typologies3['category_remote'].map(order_mapping3)).sort_values('sort_order').drop('sort_order', axis=1)\n",
    "\n",
    "# Assign an _id\n",
    "work_typologies1.loc[work_typologies1['category_travel'] == 'non_travel', 'travel_id'] = 1\n",
    "work_typologies1.loc[work_typologies1['category_travel'] == 'travel_rarely', 'travel_id'] = 2\n",
    "work_typologies1.loc[work_typologies1['category_travel'] == 'travel_frequently', 'travel_id'] = 3\n",
    "\n",
    "work_typologies2.loc[work_typologies2['category_std_hours'] == 'full_time', 'std_hours_id'] = 1\n",
    "work_typologies2.loc[work_typologies2['category_std_hours'] == 'part_time', 'std_hours_id'] = 2\n",
    "\n",
    "\n",
    "work_typologies3.loc[work_typologies3['category_remote'] == 'yes', 'remote_id'] = 1\n",
    "work_typologies3.loc[work_typologies3['category_remote'] == 'no', 'remote_id'] = 2\n",
    "\n",
    "# Round _id, allowing for NaN values\n",
    "work_typologies1['travel_id'] = work_typologies1['travel_id'].round().astype('Int64') # Use 'Int64' to handle NaN values\n",
    "work_typologies2['std_hours_id'] = work_typologies2['std_hours_id'].round().astype('Int64') # Use 'Int64' to handle NaN values\n",
    "work_typologies3['remote_id'] = work_typologies3['remote_id'].round().astype('Int64') # Use 'Int64' to handle NaN values\n",
    "\n",
    "# Combine the characteristics\n",
    "cross_joined_df1 = work_typologies1.merge(work_typologies2, how = 'cross')\n",
    "cross_joined_df2 = cross_joined_df1.merge(work_typologies3, how = 'cross')\n",
    "cross_joined_df2 = cross_joined_df2.drop_duplicates()\n",
    "\n",
    "# Create typology_id based on characteristics' categories_id\n",
    "cross_joined_df2['typology_id'] = cross_joined_df2['travel_id']*100 + cross_joined_df2['std_hours_id']*10 + cross_joined_df2['remote_id'] \n",
    "mask = pd.isna(cross_joined_df2['typology_id'])\n",
    "cross_joined_df2.loc[mask, 'typology_id'] = (cross_joined_df2.loc[mask, 'std_hours_id'] * 10 + cross_joined_df2.loc[mask, 'remote_id'])\n",
    "\n",
    "# Create typology based on characteristics' categories\n",
    "cross_joined_df2['typology'] = cross_joined_df2['category_travel'] + ', ' + cross_joined_df2['category_std_hours']+ ', ' + cross_joined_df2['category_remote'] +' remote' \n",
    "mask = pd.isna(cross_joined_df2['typology'])\n",
    "cross_joined_df2.loc[mask, 'typology'] = (cross_joined_df2.loc[mask, 'category_std_hours'] + ', ' + cross_joined_df2.loc[mask, 'category_remote'] +' remote')\n",
    "\n",
    "print(cross_joined_df2[['travel_id','std_hours_id','remote_id','typology_id','typology']])\n",
    "\n",
    "cross_joined_df2.to_csv(\"table2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c774935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3 - Satisfaction\n",
    "# Create list of the satisfaction characteristics\n",
    "categories1 = df['environment_satisfaction'].unique().tolist()\n",
    "categories2 = df['job_satisfaction'].unique().tolist()\n",
    "categories3 = df['job_involvement'].unique().tolist()\n",
    "categories4 = df['relationship_satisfaction'].unique().tolist()\n",
    "categories4 = df['work_life_balance'].unique().tolist()\n",
    "\n",
    "# Create DataFrames (df) based on the litsts\n",
    "work_typologies1 = pd.DataFrame({'category_travel': categories1})\n",
    "work_typologies2 = pd.DataFrame({'category_std_hours': categories2})\n",
    "work_typologies3 = pd.DataFrame({'category_remote': categories3})\n",
    "\n",
    "# Create a mapping order for the characteristics\n",
    "order_mapping1 = {'non_travel': 1, 'travel_rarely': 2, 'travel_frequently': 3, 'NaN': 4}\n",
    "order_mapping2 = {'full_time': 1, 'part_time': 2, 'NaN': 3}\n",
    "order_mapping3 = {'yes': 1, 'no': 2, 'NaN': 3}\n",
    "\n",
    "# Sort using custom mapping\n",
    "work_typologies1 = work_typologies1.assign(sort_order = lambda work_typologies1: work_typologies1['category_travel'].map(order_mapping1)).sort_values('sort_order').drop('sort_order', axis=1)\n",
    "work_typologies2 = work_typologies2.assign(sort_order = lambda work_typologies2: work_typologies2['category_std_hours'].map(order_mapping2)).sort_values('sort_order').drop('sort_order', axis=1)\n",
    "work_typologies3 = work_typologies3.assign(sort_order = lambda work_typologies3: work_typologies3['category_remote'].map(order_mapping3)).sort_values('sort_order').drop('sort_order', axis=1)\n",
    "\n",
    "# Assign an _id\n",
    "work_typologies1.loc[work_typologies1['category_travel'] == 'non_travel', 'travel_id'] = 1\n",
    "work_typologies1.loc[work_typologies1['category_travel'] == 'travel_rarely', 'travel_id'] = 2\n",
    "work_typologies1.loc[work_typologies1['category_travel'] == 'travel_frequently', 'travel_id'] = 3\n",
    "\n",
    "work_typologies2.loc[work_typologies2['category_std_hours'] == 'full_time', 'std_hours_id'] = 1\n",
    "work_typologies2.loc[work_typologies2['category_std_hours'] == 'part_time', 'std_hours_id'] = 2\n",
    "\n",
    "\n",
    "work_typologies3.loc[work_typologies3['category_remote'] == 'yes', 'remote_id'] = 1\n",
    "work_typologies3.loc[work_typologies3['category_remote'] == 'no', 'remote_id'] = 2\n",
    "\n",
    "# Round _id, allowing for NaN values\n",
    "work_typologies1['travel_id'] = work_typologies1['travel_id'].round().astype('Int64') # Use 'Int64' to handle NaN values\n",
    "work_typologies2['std_hours_id'] = work_typologies2['std_hours_id'].round().astype('Int64') # Use 'Int64' to handle NaN values\n",
    "work_typologies3['remote_id'] = work_typologies3['remote_id'].round().astype('Int64') # Use 'Int64' to handle NaN values\n",
    "\n",
    "# Combine the characteristics\n",
    "cross_joined_df1 = work_typologies1.merge(work_typologies2, how = 'cross')\n",
    "cross_joined_df2 = cross_joined_df1.merge(work_typologies3, how = 'cross')\n",
    "cross_joined_df2 = cross_joined_df2.drop_duplicates()\n",
    "\n",
    "# Create typology_id based on characteristics' categories_id\n",
    "cross_joined_df2['typology_id'] = cross_joined_df2['travel_id']*100 + cross_joined_df2['std_hours_id']*10 + cross_joined_df2['remote_id'] \n",
    "mask = pd.isna(cross_joined_df2['typology_id'])\n",
    "cross_joined_df2.loc[mask, 'typology_id'] = (cross_joined_df2.loc[mask, 'std_hours_id'] * 10 + cross_joined_df2.loc[mask, 'remote_id'])\n",
    "\n",
    "# Create typology based on characteristics' categories\n",
    "cross_joined_df2['typology'] = cross_joined_df2['category_travel'] + ', ' + cross_joined_df2['category_std_hours']+ ', ' + cross_joined_df2['category_remote'] +' remote' \n",
    "mask = pd.isna(cross_joined_df2['typology'])\n",
    "cross_joined_df2.loc[mask, 'typology'] = (cross_joined_df2.loc[mask, 'category_std_hours'] + ', ' + cross_joined_df2.loc[mask, 'category_remote'] +' remote')\n",
    "\n",
    "print(cross_joined_df2[['travel_id','std_hours_id','remote_id','typology_id','typology']])\n",
    "\n",
    "cross_joined_df2.to_csv(\"table2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3 - scale\n",
    "categories = df['environment_satisfaction'].unique().tolist()\n",
    "#categories\n",
    "df_scale_1_4 = pd.DataFrame({'scale_level': categories})\n",
    "#df_scale_1_4\n",
    "order_mapping = {'low': 1, 'medium': 2, 'high': 3, 'very_high': 4, 'NaN': 5}\n",
    "\n",
    "# Sort using custom mapping\n",
    "df_scale = df_scale_1_4.assign(sort_order = lambda df_scale_1_4: df_scale_1_4['scale_level'].map(order_mapping)).sort_values('sort_order').drop('sort_order', axis=1)\n",
    "\n",
    "df_scale.loc[df_scale['scale_level'] == 'low', 'scale_level_id'] = 1\n",
    "df_scale.loc[df_scale['scale_level'] == 'medium', 'scale_level_id'] = 2\n",
    "df_scale.loc[df_scale['scale_level'] == 'high', 'scale_level_id'] = 3\n",
    "df_scale.loc[df_scale['scale_level'] == 'very_high', 'scale_level_id'] = 4\n",
    "\n",
    "df_scale['scale_level_id'] = df_scale['scale_level_id'].round().astype('Int64') # Use 'Int64' to handle NaN values\n",
    "\n",
    "print(df_scale)\n",
    "\n",
    "df_scale.to_csv(\"table3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23347aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 4 - Department_role\n",
    "# Create list of the department_role categories\n",
    "categories1 = df['department'].unique().tolist()\n",
    "categories2 = df['job_level'].unique().tolist()\n",
    "categories3 = df['job_role'].unique().tolist()\n",
    "\n",
    "# Create DataFrames (df) based on the litsts\n",
    "department_role1 = pd.DataFrame({'category_department': categories1})\n",
    "department_role2 = pd.DataFrame({'category_level': categories2})\n",
    "department_role3 = pd.DataFrame({'category_role': categories3})\n",
    "\n",
    "# Create a mapping order for the characteristics\n",
    "order_mapping1 = {'research_&_development': 1, 'sales': 2, 'human_resources': 3, 'NaN': 4}\n",
    "order_mapping2 = {'entry_level': 1, 'manager': 2, 'executive': 3, 'intermediate': 4, 'senior': 5, 'NaN': 6}\n",
    "order_mapping3 = {'research_director': 1, 'manager': 2, 'sales_executive': 3, 'manufacturing_director': 4, 'research_scientist': 5,\n",
    "                    'healthcare_representative': 6, 'laboratory_technician': 7, 'sales_representative': 8, 'human_resources': 9, 'NaN': 10}\n",
    "\n",
    "# Sort using custom mapping\n",
    "department_role1 = department_role1.assign(sort_order = lambda department_role1: department_role1['category_department'].map(order_mapping1)).sort_values('sort_order').drop('sort_order', axis=1)\n",
    "department_role2 = department_role2.assign(sort_order = lambda department_role2: department_role2['category_level'].map(order_mapping2)).sort_values('sort_order').drop('sort_order', axis=1)\n",
    "department_role3 = department_role3.assign(sort_order = lambda department_role3: department_role3['category_role'].map(order_mapping3)).sort_values('sort_order').drop('sort_order', axis=1)\n",
    "\n",
    "# Assign an _id\n",
    "department_role1.loc[department_role1['category_department'] == 'research_&_development', 'department_id'] = 1\n",
    "department_role1.loc[department_role1['category_department'] == 'sales', 'department_id'] = 2\n",
    "department_role1.loc[department_role1['category_department'] == 'human_resources', 'department_id'] = 3\n",
    "\n",
    "department_role2.loc[department_role2['category_level'] == 'entry_level', 'level_id'] = 1\n",
    "department_role2.loc[department_role2['category_level'] == 'manager', 'level_id'] = 2\n",
    "department_role2.loc[department_role2['category_level'] == 'executive', 'level_id'] = 3\n",
    "department_role2.loc[department_role2['category_level'] == 'intermediate', 'level_id'] = 4\n",
    "department_role2.loc[department_role2['category_level'] == 'senior', 'level_id'] = 5\n",
    "\n",
    "department_role3.loc[department_role3['category_role'] == 'research_director', 'role_id'] = 1\n",
    "department_role3.loc[department_role3['category_role'] == 'manager', 'role_id'] = 2\n",
    "department_role3.loc[department_role3['category_role'] == 'sales_executive', 'role_id'] = 3\n",
    "department_role3.loc[department_role3['category_role'] == 'manufacturing_director', 'role_id'] = 4\n",
    "department_role3.loc[department_role3['category_role'] == 'research_scientist', 'role_id'] = 5\n",
    "department_role3.loc[department_role3['category_role'] == 'healthcare_representative', 'role_id'] = 6\n",
    "department_role3.loc[department_role3['category_role'] == 'laboratory_technician', 'role_id'] = 7\n",
    "department_role3.loc[department_role3['category_role'] == 'sales_representative', 'role_id'] = 8\n",
    "department_role3.loc[department_role3['category_role'] == 'human_resources', 'role_id'] = 9\n",
    "\n",
    "# Round _id, allowing for NaN values\n",
    "department_role1['department_id'] = department_role1['department_id'].round().astype('Int64') # Use 'Int64' to handle NaN values\n",
    "department_role2['level_id'] = department_role2['level_id'].round().astype('Int64') # Use 'Int64' to handle NaN values\n",
    "department_role3['role_id'] = department_role3['role_id'].round().astype('Int64') # Use 'Int64' to handle NaN values\n",
    "\n",
    "# Combine the characteristics\n",
    "cross_joined_df1 = department_role1.merge(department_role2, how = 'cross')\n",
    "cross_joined_df2 = cross_joined_df1.merge(department_role3, how = 'cross')\n",
    "cross_joined_df2 = cross_joined_df2.drop_duplicates()\n",
    "\n",
    "# Create typology_id based on characteristics' categories_id\n",
    "cross_joined_df2['department_role_id'] = cross_joined_df2['department_id']*100 + cross_joined_df2['level_id']*10 + cross_joined_df2['role_id'] \n",
    "mask = pd.isna(cross_joined_df2['department_role_id'])\n",
    "cross_joined_df2.loc[mask, 'department_role_id'] = (cross_joined_df2.loc[mask, 'level_id'] * 10 + cross_joined_df2.loc[mask, 'role_id'])\n",
    "\n",
    "# Create typology based on characteristics' categories\n",
    "cross_joined_df2['department_role'] = cross_joined_df2['category_department'] + ', ' + cross_joined_df2['category_level']+ ', ' + cross_joined_df2['category_role'] \n",
    "mask = pd.isna(cross_joined_df2['department_role'])\n",
    "cross_joined_df2.loc[mask, 'department_role'] = (cross_joined_df2.loc[mask, 'category_level'] + ', ' + cross_joined_df2.loc[mask, 'category_role'])\n",
    "\n",
    "print(cross_joined_df2[['department_id','level_id','role_id','department_role_id','department_role']])\n",
    "\n",
    "cross_joined_df2.to_csv(\"table4.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088dc7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'root'\n",
    "password = 'AlumnaAdalab'\n",
    "host = '127.0.0.1'\n",
    "\n",
    "# Try the connection and check the error message and code in case it is not working\n",
    "try:\n",
    "  cnx = mysql.connector.connect(user = user, password = password,\n",
    "                                host = host)\n",
    "except mysql.connector.Error as err:\n",
    "  if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "    print(\"Something is wrong with your user name or password\")\n",
    "  elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "    print(\"Database does not exist\")\n",
    "  else:\n",
    "    print(err)\n",
    "else:\n",
    "  cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'root'\n",
    "password = 'AlumnaAdalab'\n",
    "host = '127.0.0.1'\n",
    "\n",
    "# conexi√≥n (1)\n",
    "cnx = mysql.connector.connect(user = user, password = password,\n",
    "                              host = host)\n",
    "\n",
    "# cursor (2)\n",
    "mycursor = cnx.cursor()\n",
    "\n",
    "try:\n",
    "    mycursor.execute(\"CREATE DATABASE project_talent\")\n",
    "    print(mycursor)\n",
    "except mysql.connector.Error as err:\n",
    "    print(err)\n",
    "    print(\"Error Code:\", err.errno)\n",
    "    print(\"SQLSTATE\", err.sqlstate)\n",
    "    print(\"Message\", err.msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the data from artists.csv Database (LastFM) - data on artists\n",
    "engine_route = 'mysql+mysqlconnector://root:AlumnaAdalab@127.0.0.1/project_talent'\n",
    "location = 'C:\\\\Users\\\\aleiv\\\\Desktop\\\\Adalab\\\\da-promo-50-ana-leiva\\\\Modulo 3\\\\Proyecto\\\\project-da-promo-50-modulo-3-team-4\\\\df_transformado.csv'\n",
    "\n",
    "# Create the connection engine\n",
    "engine = create_engine(engine_route)\n",
    "# Uppload the CSV file in a DataFrame \n",
    "df = pd.read_csv(location)\n",
    "# Verify that the first registries are in order\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
